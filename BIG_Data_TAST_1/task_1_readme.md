# TaskÂ 1 â€“ Data Cleaning & Preparation

Welcome, machi!Â This README explains everything you need to replicate **TaskÂ 1**: cleaning the raw Kaggle dataset so itâ€™s ready for analysis in TaskÂ 2.

---

## ğŸ“‚ Project Structure

```
My_projects/
â”‚
â”œâ”€â”€ Finance_Input_Pyspark/
â”‚   â”œâ”€â”€ card_data.csv             # original Kaggle CSV(s)
â”‚   â””â”€â”€ user_data.csv
|  
â”‚â”€â”€ Finance_card_cleaned_Data/
â”‚    |---part 0000 .csv
|
|â”€â”€ Finance_User_cleaned_Data/
|    |--part 0000.csv
|
â”‚â”€â”€ Task_1_Data_clean_pyspark.py   # Spark job for TaskÂ 1
|
â””â”€â”€ README_Task1.md      # â† this file
```



## ğŸ”§ Prerequisites

| Tool                      | Recommended Version | Notes                                                            |
| ------------------------- | ------------------- | ---------------------------------------------------------------- |
| Java                      | **11Â LTS**          | `JAVA_HOME` â†’ `C:\ProgramÂ Files\Java\jdkâ€‘11`                     |
| ApacheÂ Spark              | **3.4.x**           | Standâ€‘alone install; add `%SPARK_HOME%\bin` to `Path`            |
| HadoopÂ WinUtils (Windows) | 3.2.2               | Place `winutils.exe` in `C:\hadoopâ€‘3.2.2\bin`; set `HADOOP_HOME` |
| Python                    | 3.10+               | `pip install pyspark findspark pandas`                           |

> **Environment variables** (SystemÂ â†’Â AdvancedÂ â†’Â EnvironmentÂ Variables)
>
> ```
> JAVA_HOME  = C:\ProgramÂ Files\Java\jdkâ€‘11
> HADOOP_HOME = C:\hadoopâ€‘3.2.2
> SPARK_HOME  = C:\sparkâ€‘3.4.3
> Path += %JAVA_HOME%\bin;%HADOOP_HOME%\bin;%SPARK_HOME%\bin
> ```



## ğŸƒâ€â™‚ï¸Â Running TaskÂ 1

### OptionÂ AÂ â€“Â Commandâ€‘line

```bash
spark-submit scripts/task1_clean.py \
    --input data/raw/finance_raw.csv \
    --output data/cleaned/cleaned_data.csv
```

### OptionÂ BÂ â€“Â Jupyter Notebook

1. `import findspark, os; findspark.init()`
2. Run the cells in `task1_clean.ipynb` (same logic as the script).



## ğŸ§¹ Cleaning Steps Implemented

| Step | Transformation                                                                                                        |
| ---- | --------------------------------------------------------------------------------------------------------------------- |
| 1    | **Load** raw CSV with `header=True`, `inferSchema=True`                                                               |
| 2    | **Currency cleanup** â€“ remove `$` and `,` from `per_capita_income` using `regexp_replace`, then cast to `IntegerType` |
| 3    | **Date parsing** â€“ convert `expires`/`date` strings to `TimestampType` via `to_timestamp()`                           |
| 4    | **Null handling** â€“ counted nulls per column; dropped or imputed where required                                       |
| 5    | **Column pruning / renaming** â€“ kept only relevant features and renamed to snake\_case                                |
| 6    | **Write** cleaned DataFrame with `coalesce(1)`Â +Â `mode("overwrite")` to the `data/cleaned` folder                     |



## âœ…Â Expected Output

- `data/cleaned/cleaned_data.csv` (single file)
- Sample schema:
  ```
  root
   |-- state: string                       (nullable = true)
   |-- per_capita_income: integer          (nullable = true)
   |-- expires: timestamp                  (nullable = true)
   |-- year: integer                       (nullable = true)
  ```



## ğŸ›  Troubleshooting Tips

| Symptom                              | Fix                                                           |
| ------------------------------------ | ------------------------------------------------------------- |
| `JavaPackage object is not callable` | Confirm JavaÂ 11 and `%JAVA_HOME%` set, restart kernel         |
| `Py4JJavaError` when writing         | Use raw string paths (`r"D:\â€¦"`), avoid OneDrive sync folders |
| `winutils.exe` not found             | Ensure `C:\hadoopâ€‘3.2.2\bin` in `Path` & file present         |


